{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f0f41d3",
   "metadata": {},
   "source": [
    "In this notebook, we make API requests to OpenRouter, to generate Gherkins from user stories using various models and various prompting approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05c53d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import chardet # To detect file encodings\n",
    "import asyncio\n",
    "import httpx\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "or_token = os.getenv(\"openrouter_token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e8cbbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "us_data_dir = Path(\"../data/user_stories/sample_data\")\n",
    "experiment_dir = Path(\"../data/gherkins/sample_data/test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c373f35",
   "metadata": {},
   "source": [
    "#### Load user story data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bfadf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to parse user stories from a .txt file or all .txt files in a folder\n",
    "def parse_user_stories(path):\n",
    "    user_stories = {}\n",
    "    \n",
    "    if os.path.isfile(path):\n",
    "        files = [path] # Single .txt file\n",
    "    elif os.path.isdir(path):\n",
    "        files = [os.path.join(path, f) for f in os.listdir(path) if f.endswith('.txt')] # All .txt files in folder\n",
    "    else:\n",
    "        raise ValueError(f\"Path '{path}' is not a valid file or folder.\")\n",
    "        \n",
    "    for filepath in files:\n",
    "        filename = os.path.splitext(os.path.basename(filepath))[0] # Remove extension\n",
    "\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            for i, line in enumerate(f, start=1):\n",
    "                us = line.strip()\n",
    "                if us:  # Skip empty lines\n",
    "                    us_id = f\"{filename}_{i}\"\n",
    "                    user_stories[us_id] = us\n",
    "\n",
    "    return user_stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "930e10d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create user story dictionary, creating ID keys based on filename and line number\n",
    "us_dict = parse_user_stories(us_data_dir / \"g04-recycling.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4500ca28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(us_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5507ea9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for duplicate user stories\n",
    "us_list = list(us_dict.values())\n",
    "\n",
    "len(us_list) == len(set(us_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b867413",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'g04-recycling_1': 'As a user, I want to click on the address, so that it takes me to a new tab with Google Maps.',\n",
       " 'g04-recycling_2': 'As a user, I want to be able to anonymously view public information, so that I know about recycling centers near me before creating an account.',\n",
       " 'g04-recycling_3': 'As a user, I want to be able to enter my zip code and get a list of nearby recycling facilities, so that I can determine which ones I should consider.'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create sample subset of user stories for testing\n",
    "sample_us_dict = dict(list(us_dict.items())[:3])\n",
    "\n",
    "sample_us_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693ad457",
   "metadata": {},
   "source": [
    "#### Create functions to make single-turn API requests.\n",
    "\n",
    "Simulates creating a new chat for each user story, so each user story is processed by the LLM in isolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63ffa868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to format model name\n",
    "def format_model_name(model):\n",
    "    return model.replace('/', '-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e504bf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to set up request parameters\n",
    "def build_openrouter_request_data(model, or_token, us_text, system_prompt=None, temperature=0.8):\n",
    "    url = \"https://openrouter.ai/api/v1/chat/completions\"\n",
    "    \n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {or_token}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [],\n",
    "        \"temperature\": temperature,\n",
    "        \"provider\": {\n",
    "            \"data_collection\": \"deny\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    if system_prompt is not None:\n",
    "        data[\"messages\"].append({\"role\": \"system\", \"content\": system_prompt})\n",
    "\n",
    "    data[\"messages\"].append({\"role\": \"user\", \"content\": f\"User story: {us_text}\"})\n",
    "\n",
    "    return url, headers, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43cb2a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make OpenRouter API request \n",
    "# NOTE: 1) this function uses the standard Requests library, so is blocking/synchronous, i.e., each request waits for previous to complete\n",
    "# NOTE: 2) this function simulates creating a new chat for each user story\n",
    "def openrouter_request(model, or_token, us_id, us_text, system_prompt=None, temperature=0.8):\n",
    "    url, headers, data = build_openrouter_request_data(model, or_token, us_text, system_prompt, temperature)\n",
    "\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, json=data)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        created = response.json().get(\"created\", \"\")\n",
    "        response_content = response.json().get(\"choices\")[0][\"message\"][\"content\"]\n",
    "        prompt_tokens = response.json().get(\"usage\", {}).get(\"prompt_tokens\", 0)\n",
    "        completion_tokens = response.json().get(\"usage\", {}).get(\"completion_tokens\", 0)\n",
    "\n",
    "        return {\n",
    "            \"model\": format_model_name(model),\n",
    "            \"app_id\": us_id.split('_')[0],\n",
    "            \"system_prompt\": system_prompt,\n",
    "            \"us_id\": us_id,\n",
    "            \"user_prompt\": us_text, # In this case, the user prompt is the user story (there is no added preamble)\n",
    "            \"ai_response\": response_content,\n",
    "            \"prompt_tokens\": prompt_tokens,\n",
    "            \"completion_tokens\": completion_tokens,\n",
    "            \"response_created\": created\n",
    "        }\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Sync error: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc3c651d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make asynchronous OpenRouter API request\n",
    "# NOTE: 1) this function uses the httpx library for async requests, i.e., it does not block while waiting for a response\n",
    "# NOTE: 2) as above, this function simulates creating a new chat for each user story\n",
    "async def openrouter_request_async(model, or_token, us_id, us_text, system_prompt=None, temperature=0.8):\n",
    "    url, headers, data = build_openrouter_request_data(model, or_token, us_text, system_prompt, temperature)\n",
    "\n",
    "    try:\n",
    "        async with httpx.AsyncClient() as client:\n",
    "            response = await client.post(url, headers=headers, json=data)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            json_data = response.json()\n",
    "            created = json_data.get(\"created\", \"\")\n",
    "            response_content = json_data.get(\"choices\")[0][\"message\"][\"content\"]\n",
    "            prompt_tokens = json_data.get(\"usage\", {}).get(\"prompt_tokens\", 0)\n",
    "            completion_tokens = json_data.get(\"usage\", {}).get(\"completion_tokens\", 0)\n",
    "\n",
    "            return {\n",
    "                \"model\": format_model_name(model),\n",
    "                \"app_id\": us_id.split('_')[0],\n",
    "                \"system_prompt\": system_prompt,\n",
    "                \"reminder\": None, # Reminder is only used in multi-turn chats\n",
    "                \"us_id\": us_id,\n",
    "                \"user_prompt\": us_text, # In this case, the user prompt is the user story (there is no added preamble)\n",
    "                \"ai_response\": response_content,\n",
    "                \"prompt_tokens\": prompt_tokens,\n",
    "                \"completion_tokens\": completion_tokens,\n",
    "                \"response_created\": created\n",
    "            }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Async error: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8b14c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "semaphore = asyncio.Semaphore(5)  # Limit concurrent requests to 5 to avoid rate limiting\n",
    "\n",
    "# Function to make limited concurrent asynchronous OpenRouter API requests\n",
    "async def limited_openrouter_request(model, or_token, us_id, us_text, system_prompt=None, temperature=0.8):\n",
    "    async with semaphore:\n",
    "        return await openrouter_request_async(model, or_token, us_id, us_text, system_prompt, temperature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42d43d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are a QA Engineer. Please generate a complete Gherkin feature file with 3-5 realistic, testable scenarios for the user story below. Please return the Gherkin only, without comments or explanations.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50e02c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llms = [\"openai/gpt-4o-mini\", \"meta-llama/llama-3.1-70b-instruct\"]\n",
    "llms = [\"openai/gpt-4o-mini\", \"google/gemini-2.0-flash-001\"]\n",
    "\n",
    "# Main function to orchestrate asynchronous OpenRouter API requests\n",
    "async def main():\n",
    "    tasks = [\n",
    "        limited_openrouter_request(model, or_token, us_id, user_story, system_prompt=system_prompt)\n",
    "        for us_id, user_story in us_dict.items()\n",
    "        for model in llms\n",
    "    ]\n",
    "\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    return results\n",
    "\n",
    "results = await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1247d4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_records(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bdc5631c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>app_id</th>\n",
       "      <th>system_prompt</th>\n",
       "      <th>reminder</th>\n",
       "      <th>us_id</th>\n",
       "      <th>user_prompt</th>\n",
       "      <th>ai_response</th>\n",
       "      <th>prompt_tokens</th>\n",
       "      <th>completion_tokens</th>\n",
       "      <th>response_created</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>openai-gpt-4o-mini</td>\n",
       "      <td>g04-recycling</td>\n",
       "      <td>You are a QA Engineer. Please generate a compl...</td>\n",
       "      <td>None</td>\n",
       "      <td>g04-recycling_1</td>\n",
       "      <td>As a user, I want to click on the address, so ...</td>\n",
       "      <td>```gherkin\\nFeature: Open Google Maps from add...</td>\n",
       "      <td>83</td>\n",
       "      <td>250</td>\n",
       "      <td>1762428981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>google-gemini-2.0-flash-001</td>\n",
       "      <td>g04-recycling</td>\n",
       "      <td>You are a QA Engineer. Please generate a compl...</td>\n",
       "      <td>None</td>\n",
       "      <td>g04-recycling_1</td>\n",
       "      <td>As a user, I want to click on the address, so ...</td>\n",
       "      <td>```gherkin\\nFeature: Address Link Opens Google...</td>\n",
       "      <td>70</td>\n",
       "      <td>219</td>\n",
       "      <td>1762428981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>openai-gpt-4o-mini</td>\n",
       "      <td>g04-recycling</td>\n",
       "      <td>You are a QA Engineer. Please generate a compl...</td>\n",
       "      <td>None</td>\n",
       "      <td>g04-recycling_2</td>\n",
       "      <td>As a user, I want to be able to anonymously vi...</td>\n",
       "      <td>```gherkin\\nFeature: Anonymous viewing of publ...</td>\n",
       "      <td>87</td>\n",
       "      <td>272</td>\n",
       "      <td>1762428981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>google-gemini-2.0-flash-001</td>\n",
       "      <td>g04-recycling</td>\n",
       "      <td>You are a QA Engineer. Please generate a compl...</td>\n",
       "      <td>None</td>\n",
       "      <td>g04-recycling_2</td>\n",
       "      <td>As a user, I want to be able to anonymously vi...</td>\n",
       "      <td>```gherkin\\nFeature: Anonymous User Can View P...</td>\n",
       "      <td>74</td>\n",
       "      <td>355</td>\n",
       "      <td>1762428981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>openai-gpt-4o-mini</td>\n",
       "      <td>g04-recycling</td>\n",
       "      <td>You are a QA Engineer. Please generate a compl...</td>\n",
       "      <td>None</td>\n",
       "      <td>g04-recycling_3</td>\n",
       "      <td>As a user, I want to be able to enter my zip c...</td>\n",
       "      <td>```gherkin\\nFeature: Nearby Recycling Faciliti...</td>\n",
       "      <td>92</td>\n",
       "      <td>363</td>\n",
       "      <td>1762428981</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         model         app_id  \\\n",
       "0           openai-gpt-4o-mini  g04-recycling   \n",
       "1  google-gemini-2.0-flash-001  g04-recycling   \n",
       "2           openai-gpt-4o-mini  g04-recycling   \n",
       "3  google-gemini-2.0-flash-001  g04-recycling   \n",
       "4           openai-gpt-4o-mini  g04-recycling   \n",
       "\n",
       "                                       system_prompt reminder  \\\n",
       "0  You are a QA Engineer. Please generate a compl...     None   \n",
       "1  You are a QA Engineer. Please generate a compl...     None   \n",
       "2  You are a QA Engineer. Please generate a compl...     None   \n",
       "3  You are a QA Engineer. Please generate a compl...     None   \n",
       "4  You are a QA Engineer. Please generate a compl...     None   \n",
       "\n",
       "             us_id                                        user_prompt  \\\n",
       "0  g04-recycling_1  As a user, I want to click on the address, so ...   \n",
       "1  g04-recycling_1  As a user, I want to click on the address, so ...   \n",
       "2  g04-recycling_2  As a user, I want to be able to anonymously vi...   \n",
       "3  g04-recycling_2  As a user, I want to be able to anonymously vi...   \n",
       "4  g04-recycling_3  As a user, I want to be able to enter my zip c...   \n",
       "\n",
       "                                         ai_response  prompt_tokens  \\\n",
       "0  ```gherkin\\nFeature: Open Google Maps from add...             83   \n",
       "1  ```gherkin\\nFeature: Address Link Opens Google...             70   \n",
       "2  ```gherkin\\nFeature: Anonymous viewing of publ...             87   \n",
       "3  ```gherkin\\nFeature: Anonymous User Can View P...             74   \n",
       "4  ```gherkin\\nFeature: Nearby Recycling Faciliti...             92   \n",
       "\n",
       "   completion_tokens  response_created  \n",
       "0                250        1762428981  \n",
       "1                219        1762428981  \n",
       "2                272        1762428981  \n",
       "3                355        1762428981  \n",
       "4                363        1762428981  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f8e30579",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102, 10)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a5fe8694",
   "metadata": {},
   "outputs": [],
   "source": [
    "app_id = df[\"app_id\"].iloc[0]\n",
    "\n",
    "df.to_csv(experiment_dir / f\"{app_id}_single_turn_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f001410",
   "metadata": {},
   "source": [
    "#### Create functions to make multi-turn API requests.\n",
    "\n",
    "Simulating an ongoing LLM chat interaction, where the chat history is appended to the start of each new user chat turn &mdash; so multiple user stories can be process in the same chat interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ad8f8ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create chat logs directory\n",
    "chat_log_dir = experiment_dir / \"chat_logs\"\n",
    "os.makedirs(chat_log_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3708c792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get path for backup chat log file with timestamp\n",
    "# def backup_file_path(chat_log_dir, model):\n",
    "#     timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "#     file_path = chat_log_dir / model / f\"chat_log_{timestamp}.json\"\n",
    "\n",
    "#     return file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e29d885a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to save multi-turn chat to active log file\n",
    "def save_conversation(log_dir, conversation):\n",
    "    filepath = log_dir / \"active.json\"\n",
    "\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir, exist_ok=True)\n",
    "        \n",
    "    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(conversation, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    return filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "965ba338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to back up active chat log file\n",
    "def backup_active_file(log_dir):\n",
    "    active_file_path = log_dir / \"active.json\"\n",
    "\n",
    "    if os.path.exists(active_file_path):\n",
    "        backup_file_path = log_dir / f\"chat_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "        shutil.copy(active_file_path, backup_file_path)\n",
    "\n",
    "        return backup_file_path\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "64d0e03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = \"You are a QA Engineer. For each user story I give you, please generate a complete Gherkin feature file with at least three realistic, testable scenarios. Try to cover: 1. The happy path (expected successful flow), 2. At least one edge case, 3. At least one error or failure condition. Please return the Gherkin only, without comments or explanation.\"\n",
    "# reminder = \"Reminder: your task is to generate a complete Gherkin feature file with at least three realistic, testable scenarios for the user story I give you, returning only the Gherkin.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2c7ab330",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are a QA Engineer. For each user story I give you, please generate a complete Gherkin feature file with 3-5 realistic, testable scenarios. Please return the Gherkin only, without comments or explanation.\"\n",
    "reminder = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "80e8577c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to conduct multi-turn chat with model over multiple user stories and log the conversation\n",
    "async def chat_with_model(model, or_token, log_dir, user_stories, system_prompt=None, reminder=None, temperature=0.8):\n",
    "    # Create model-specific log directory, if it doesn't exist\n",
    "    model_log_dir = log_dir / format_model_name(model)\n",
    "    os.makedirs(model_log_dir, exist_ok=True)\n",
    "\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {or_token}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    print(\"Starting chat for model:\", model)\n",
    "\n",
    "    # Messages list will hold the full conversation history, updated after each turn\n",
    "    messages = []\n",
    "\n",
    "    # Conversation log dict will hold the structured log to be saved to file\n",
    "    conversation_log = {\n",
    "        \"model\": model,\n",
    "        \"system_prompt\": system_prompt,\n",
    "        \"conversation_turns\": []\n",
    "    }\n",
    "\n",
    "    # Completed stories will hold us_ids of already processed user stories\n",
    "    completed_stories = []\n",
    "\n",
    "    # Add system prompt to messages - prompt is sent once at start of conversation\n",
    "    if system_prompt is not None:\n",
    "        messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "        conversation_log[\"system_prompt\"] = system_prompt\n",
    "\n",
    "    async with httpx.AsyncClient() as client:\n",
    "        for us_id, us_text in user_stories.items():\n",
    "            if us_id in completed_stories: # This is probably redundant now but will be useful when resuming from file (functionality to be added)\n",
    "                print(f\"Skipping already completed user story: {us_id}\")\n",
    "                continue\n",
    "\n",
    "            turn = {\"user\": {}, \"assistant\": {}}\n",
    "\n",
    "            if reminder is not None:\n",
    "                # Add reminder message before each user story\n",
    "                messages.append({\"role\": \"user\", \"content\": reminder})\n",
    "                # conversation_log[\"conversation_turns\"].append({\"role\": \"user\", \"content\": reminder}) # Log user messages\n",
    "                turn[\"user\"][\"reminder\"] = reminder\n",
    "            \n",
    "            user_message = f\"User story: {us_text}\"\n",
    "            \n",
    "            messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "            # conversation_log[\"conversation_turns\"].append({\"role\": \"user\", \"content\": user_message, \"us_id\": us_id}) # Log user messages\n",
    "\n",
    "            turn[\"user\"][\"content\"] = user_message\n",
    "            turn[\"user\"][\"us_id\"] = us_id\n",
    "\n",
    "            try:\n",
    "                response = await client.post(\n",
    "                    url=\"https://openrouter.ai/api/v1/chat/completions\",\n",
    "                    headers=headers,\n",
    "                    json={\n",
    "                        \"model\": model,\n",
    "                        \"messages\": messages,\n",
    "                        \"temperature\": temperature,\n",
    "                        \"provider\": {\"data_collection\": \"deny\"},\n",
    "                    },\n",
    "                )\n",
    "                \n",
    "                response.raise_for_status()\n",
    "\n",
    "                data = response.json()\n",
    "                \n",
    "                assistant_response = data[\"choices\"][0][\"message\"][\"content\"]\n",
    "                created = data.get(\"created\", \"\")\n",
    "                prompt_tokens = data.get(\"usage\", {}).get(\"prompt_tokens\", 0)\n",
    "                completion_tokens = data.get(\"usage\", {}).get(\"completion_tokens\", 0)\n",
    "\n",
    "                messages.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
    "                # conversation_log[\"conversation_turns\"].append({\"role\": \"assistant\", \"content\": assistant_response, \"created\": created, \"prompt_tokens\": prompt_tokens, \"completion_tokens\": completion_tokens}) # Log assistant messages\n",
    "\n",
    "                turn[\"assistant\"][\"content\"] = assistant_response\n",
    "                turn[\"assistant\"][\"created\"] = created\n",
    "                turn[\"assistant\"][\"prompt_tokens\"] = prompt_tokens\n",
    "                turn[\"assistant\"][\"completion_tokens\"] = completion_tokens\n",
    "\n",
    "                completed_stories.append(us_id)\n",
    "\n",
    "                conversation_log[\"conversation_turns\"].append(turn)\n",
    "\n",
    "                # Save updated conversation log to active file after each turn\n",
    "                save_conversation(model_log_dir, conversation_log)\n",
    "                # print(messages)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error during API request for user story {us_id}: {e}\")\n",
    "                continue\n",
    "\n",
    "    # Save time-stamped backup of active chat log file\n",
    "    backup_active_file(model_log_dir)\n",
    "    \n",
    "    return conversation_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1bf6037c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting chat for model: openai/gpt-4o-mini\n",
      "Starting chat for model: google/gemini-2.0-flash-001\n"
     ]
    }
   ],
   "source": [
    "async def main():\n",
    "    results = await asyncio.gather(*[\n",
    "        chat_with_model(model, or_token, chat_log_dir, us_dict, system_prompt=system_prompt)\n",
    "        for model in llms\n",
    "    ])\n",
    "    return results\n",
    "\n",
    "results = await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22523a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conversation_results_to_df(results):\n",
    "    rows = []\n",
    "\n",
    "    for conv in results:\n",
    "        model = conv.get(\"model\")\n",
    "        system_prompt = conv.get(\"system_prompt\")\n",
    "\n",
    "        for turn in conv.get(\"conversation_turns\", []):\n",
    "            user = turn.get(\"user\", {})\n",
    "            assistant = turn.get(\"assistant\", {})\n",
    "\n",
    "            rows.append({\n",
    "                \"model\": format_model_name(model),\n",
    "                \"app_id\": user.get(\"us_id\").split('_')[0],\n",
    "                \"system_prompt\": system_prompt,\n",
    "                \"reminder\": user.get(\"reminder\", \"\"),\n",
    "                \"us_id\": user.get(\"us_id\").split('_')[1],\n",
    "                \"user_prompt\": user.get(\"content\"),\n",
    "                \"ai_response\": assistant.get(\"content\"),\n",
    "                \"prompt_tokens\": assistant.get(\"prompt_tokens\"),\n",
    "                \"completion_tokens\": assistant.get(\"completion_tokens\"),\n",
    "                \"response_created\": assistant.get(\"created\")\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "22c1deeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = conversation_results_to_df(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a6cbe933",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>app_id</th>\n",
       "      <th>system_prompt</th>\n",
       "      <th>reminder</th>\n",
       "      <th>us_id</th>\n",
       "      <th>user_prompt</th>\n",
       "      <th>ai_response</th>\n",
       "      <th>prompt_tokens</th>\n",
       "      <th>completion_tokens</th>\n",
       "      <th>response_created</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>openai-gpt-4o-mini</td>\n",
       "      <td>g04-recycling</td>\n",
       "      <td>You are a QA Engineer. For each user story I g...</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>User story: As a user, I want to click on the ...</td>\n",
       "      <td>```gherkin\\nFeature: Open address in Google Ma...</td>\n",
       "      <td>86</td>\n",
       "      <td>250</td>\n",
       "      <td>1762429290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>openai-gpt-4o-mini</td>\n",
       "      <td>g04-recycling</td>\n",
       "      <td>You are a QA Engineer. For each user story I g...</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>User story: As a user, I want to be able to an...</td>\n",
       "      <td>```gherkin\\nFeature: Anonymous viewing of publ...</td>\n",
       "      <td>376</td>\n",
       "      <td>279</td>\n",
       "      <td>1762429295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>openai-gpt-4o-mini</td>\n",
       "      <td>g04-recycling</td>\n",
       "      <td>You are a QA Engineer. For each user story I g...</td>\n",
       "      <td></td>\n",
       "      <td>3</td>\n",
       "      <td>User story: As a user, I want to be able to en...</td>\n",
       "      <td>```gherkin\\nFeature: Retrieve nearby recycling...</td>\n",
       "      <td>700</td>\n",
       "      <td>327</td>\n",
       "      <td>1762429302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>openai-gpt-4o-mini</td>\n",
       "      <td>g04-recycling</td>\n",
       "      <td>You are a QA Engineer. For each user story I g...</td>\n",
       "      <td></td>\n",
       "      <td>4</td>\n",
       "      <td>User story: As a user, I want to be able to ge...</td>\n",
       "      <td>```gherkin\\nFeature: View hours of recycling f...</td>\n",
       "      <td>1073</td>\n",
       "      <td>311</td>\n",
       "      <td>1762429309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>openai-gpt-4o-mini</td>\n",
       "      <td>g04-recycling</td>\n",
       "      <td>You are a QA Engineer. For each user story I g...</td>\n",
       "      <td></td>\n",
       "      <td>5</td>\n",
       "      <td>User story: As a user, I want to have a flexib...</td>\n",
       "      <td>```gherkin\\nFeature: Flexible pick-up time for...</td>\n",
       "      <td>1419</td>\n",
       "      <td>345</td>\n",
       "      <td>1762429316</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                model         app_id  \\\n",
       "0  openai-gpt-4o-mini  g04-recycling   \n",
       "1  openai-gpt-4o-mini  g04-recycling   \n",
       "2  openai-gpt-4o-mini  g04-recycling   \n",
       "3  openai-gpt-4o-mini  g04-recycling   \n",
       "4  openai-gpt-4o-mini  g04-recycling   \n",
       "\n",
       "                                       system_prompt reminder us_id  \\\n",
       "0  You are a QA Engineer. For each user story I g...              1   \n",
       "1  You are a QA Engineer. For each user story I g...              2   \n",
       "2  You are a QA Engineer. For each user story I g...              3   \n",
       "3  You are a QA Engineer. For each user story I g...              4   \n",
       "4  You are a QA Engineer. For each user story I g...              5   \n",
       "\n",
       "                                         user_prompt  \\\n",
       "0  User story: As a user, I want to click on the ...   \n",
       "1  User story: As a user, I want to be able to an...   \n",
       "2  User story: As a user, I want to be able to en...   \n",
       "3  User story: As a user, I want to be able to ge...   \n",
       "4  User story: As a user, I want to have a flexib...   \n",
       "\n",
       "                                         ai_response  prompt_tokens  \\\n",
       "0  ```gherkin\\nFeature: Open address in Google Ma...             86   \n",
       "1  ```gherkin\\nFeature: Anonymous viewing of publ...            376   \n",
       "2  ```gherkin\\nFeature: Retrieve nearby recycling...            700   \n",
       "3  ```gherkin\\nFeature: View hours of recycling f...           1073   \n",
       "4  ```gherkin\\nFeature: Flexible pick-up time for...           1419   \n",
       "\n",
       "   completion_tokens  response_created  \n",
       "0                250        1762429290  \n",
       "1                279        1762429295  \n",
       "2                327        1762429302  \n",
       "3                311        1762429309  \n",
       "4                345        1762429316  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "66f3086d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102, 10)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ca32b152",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "story_counts = results_df[\"us_id\"].value_counts()\n",
    "missing_stories = story_counts[story_counts < 2].index.tolist()\n",
    "\n",
    "missing_stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "659dbdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(experiment_dir / f\"{app_id}_multi_turn_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57d190c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a793e81d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gherkin_eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
