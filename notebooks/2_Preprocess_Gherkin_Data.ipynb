{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "Receives input from Generate_Gherkins\n",
    "\n",
    "INPUT: raw generated gherkins (model, timestamp, us_id, user_story, assistant_response, prompt_tokens, completion_tokens, created)\n",
    "- created is the unix timestamp returned by the model for when the request was processed\n",
    "- timestamp is generated in our code when the request is made, as a back up in case created is null\n",
    "\n",
    "REQUIRED OUTPUT FORMAT: \n",
    "For input to METEOR/TF-IDF/SentenceTransformer: us_id, us_text, scenario_title, model, scenario_text, scenario_id - scenario_text is the full scenario text, (unsure if we want feature info, e.g. title, description)\n",
    "Other outputs:\n",
    "- Above but for step data\n",
    "- Parse error data\n",
    "- Lint report data\n",
    "\n",
    "Combining Sample_Data_Base_Preprocessing and Pipeline? Or do we want this to be generic to all data and still have a sample data base preprocess? I think all generated data will be the same/can be made the same so if we need separate for human data we can make that later.\n",
    "\n",
    "All outputs:\n",
    "- Feature files\n",
    "- Full scenario data for traceability scoring (this is the input?) X\n",
    "- Step data (for clustering?)\n",
    "- Parser error data (parser is used to create above set)\n",
    "- Lint report data\n",
    "\n",
    "\n",
    "Completed outputs:\n",
    "1. Feature file for each ai_response, written to `gherkins/sample_data/<exp_label>/feature_files/<model>/<app>` directory\n",
    "2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from pathlib import Path\n",
    "\n",
    "from importlib import reload\n",
    "import config\n",
    "reload(config)\n",
    "\n",
    "from config import DATASET_NAME, EXPERIMENT_NAME, INPUT_DATA_PATH, GENERATION_TECHNIQUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_dir = Path(f\"../data/{DATASET_NAME}/experiment_outputs/{EXPERIMENT_NAME}/{GENERATION_TECHNIQUE}/\")\n",
    "exp_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "input_file_name = f\"{GENERATION_TECHNIQUE}_raw_results.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the raw BDD dataset, containing the model outputs\n",
    "raw_df = pd.read_csv(exp_dir / input_file_name) # TODO: store prompts? Important for multi-turn chats where we should record the order of presentation of user stories.\n",
    "\n",
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporary changes that I have fixed in the data generation code (but don't want to rerun generation)\n",
    "raw_df.rename(columns={'user_prompt': 'us_text'}, inplace=True)\n",
    "\n",
    "raw_df[\"us_id\"] = raw_df[\"us_id\"].str.split('_').str[1]\n",
    "\n",
    "raw_df[\"us_id\"] = raw_df[\"us_id\"].astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates (on model and us_id)\n",
    "duplicates = raw_df[raw_df.duplicated(subset=[\"model\", \"us_id\"], keep=False)]\n",
    "\n",
    "print(duplicates.shape[0], \"duplicate rows found:\")\n",
    "\n",
    "duplicates.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "raw_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check a response has been generated for each user story by each model (number of rows should equal number of unique user stories * number of unique models)\n",
    "print(\"Number of rows in raw_df:\", raw_df.shape[0])\n",
    " \n",
    "raw_df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove triple backticks and language specifiers from \n",
    "def remove_padding(input_string):\n",
    "    match = re.search(r\"```[\\w]*\\n(.*?)\\n```\", input_string, re.DOTALL)\n",
    "\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    \n",
    "    return input_string.strip()\n",
    "\n",
    "raw_df['ai_response'] = raw_df['ai_response'].apply(remove_padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "<b>Create Feature Files</b>\n",
    "\n",
    "At this point, we write each `ai_response` to its own feature file, then parse and lint those files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and write feature file for each ai_response\n",
    "# TODO: fix this for directory structure\n",
    "def write_feature_file(record, experiment_dir):\n",
    "    model = record['model']\n",
    "    app_id = record['app_id']\n",
    "    us_id = record['us_id']\n",
    "\n",
    "    feature_content = record['ai_response']\n",
    "\n",
    "    filename = f\"{app_id}_{model}_{us_id}\"\n",
    "\n",
    "    feature_dir = experiment_dir / \"features\" / model\n",
    "    feature_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    feature_file_path = feature_dir / f\"{filename}.feature\"\n",
    "\n",
    "    try:   \n",
    "        with open(feature_file_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(feature_content.strip())\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error writing {feature_file_path}: {e}\")\n",
    "\n",
    "# for index, row in raw_df.iterrows():\n",
    "#     write_feature_file(row, exp_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "<b>Review and Process Parsed Data</b>\n",
    "\n",
    "Next, we read and review the parsed gherkin step data (generated in Gherkin_Parser.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read parsed step data from gherkin parser output\n",
    "parse_df = pd.read_csv(exp_dir / 'parsed_step_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse_df contains a record per step\n",
    "parse_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add us_text to parse_df by merging with raw_df on model, app_id, us_id\n",
    "parse_df = parse_df.merge(raw_df[['model', 'app_id', 'us_id', 'us_text']], on=['model', 'app_id', 'us_id'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_df.to_csv(exp_dir / 'processed_step_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "<b>Review `gherkin-lint` Reports</b>\n",
    "\n",
    "Read and review reports generated by `gherkin-lint`.\n",
    "\n",
    "TODO: perform this in another notebook and read results here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "<b>Create Full Scenario Dataset for Traceability Evaluation</b>\n",
    "\n",
    "Next, we use the parsed data to create a dataset of complete scenarios (joining the parsed steps) to use in computing similarity between user stories and gherkins, in our traceability experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_step_df = parse_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_step_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all rows for us_ids that have error == True, i.e. where parsing failed for one or more model's output (to maintain a matched-pair dataset for traceability experiments)\n",
    "error_us_ids = processed_step_df.loc[processed_step_df['error'] == True, 'us_id'].unique()\n",
    "processed_step_df = processed_step_df[~processed_step_df['us_id'].isin(error_us_ids)].reset_index(drop=True)\n",
    "\n",
    "processed_step_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: also remove user stories that resulted in gherkins with gherkin lint errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that for each model's output, us-feature mapping is one-to-one\n",
    "us_feature_counts = processed_step_df.groupby(['model', 'us_id'])['feature_name'].nunique()\n",
    "\n",
    "us_feature_counts[us_feature_counts > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign unique numeric scenario_id to each scenario_name within each model and us_id\n",
    "processed_step_df['scenario_id'] = processed_step_df.groupby(['model', 'us_id'])['scenario_name'].transform(lambda x: pd.factorize(x)[0] + 1)\n",
    "processed_step_df['scenario_id'] = processed_step_df[\"model\"] + \"_\" + processed_step_df[\"us_id\"].astype(str) + \"_\" + processed_step_df['scenario_id'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_step_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_step_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_step_df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_step(row):\n",
    "    step_text = f\"{row['step_keyword']} {row['step_text']}\"\n",
    "\n",
    "    if pd.notna(row['step_data_table']):\n",
    "        for table_row in row['step_data_table']:\n",
    "            step_text += \" | \" + \" | \".join(table_row)\n",
    "        step_text += \" | \"\n",
    "\n",
    "    if pd.notna(row['step_doc_string']):\n",
    "        step_text += f\" \\\"\\\"\\\" {row['step_doc_string']} \\\"\\\"\\\" \"\n",
    "\n",
    "    return step_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_step_df['flat_step'] = processed_step_df.apply(flatten_step, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_step_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_scenarios = (\n",
    "    processed_step_df.groupby(['app_id', 'model', 'us_id', 'scenario_id'])\n",
    "    .agg({\n",
    "        'flat_step': lambda steps: \" \".join(steps),  # join all steps\n",
    "        'feature_name': 'first',\n",
    "        'scenario_name': 'first',\n",
    "        'scenario_examples': 'first',\n",
    "        'us_text': 'first'\n",
    "    })\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "full_scenarios.rename(columns={'flat_step': 'scenario_text'}, inplace=True)\n",
    "\n",
    "# TODO: keep scenario description?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_scenarios.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_scenarios.to_csv(exp_dir / 'parsed_scenario_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "<b>Process Step Dataset for Step-Based Traceability Experiments</b>\n",
    "\n",
    "`flat_step` combines `step_text`, `step_keyword`, `step_data_table`, and `step_doc_string`, where present in the case of the latter two. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_step_df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_step_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign unique numeric step_id to each step within each scenario_id, model, and us_id\n",
    "processed_step_df['step_id'] = processed_step_df.groupby(['model', 'us_id', \"scenario_id\"])['flat_step'].transform(lambda x: pd.factorize(x)[0] + 1)\n",
    "processed_step_df['step_id'] = processed_step_df['scenario_id'].astype(str) + \"_\" + processed_step_df['step_id'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_step_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_step_df.drop(columns=['filepath', 'feature_keyword', 'feature_tags', 'rule_name', 'rule_description', 'rule_tags', 'scenario_keyword', 'scenario_tags', 'step_keyword', 'step_text', 'step_data_table', 'step_doc_string', 'error'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_step_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_step_df.to_csv(exp_dir / 'processed_step_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gherkin_eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
