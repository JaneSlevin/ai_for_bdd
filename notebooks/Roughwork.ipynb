{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "Temporary notebook, to preprocess data so it is in correct format for input to various notebooks. This preprocessing will be done in the preceding step in the chain but haven't added the cleaned up notebooks for each step yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the raw BDD dataset, containing the model outputs\n",
    "df = pd.read_csv('../data/gherkins/exp2_p1_g04_10-10-25_first_5_us.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'user_story': 'us_text', 'created': 'timestamp_response_generated', 'assistant_response': 'ai_response'}, inplace=True)\n",
    "\n",
    "df.drop(columns=['timestamp'], inplace=True)\n",
    "df['app_id'] = df.us_id.str.split('_').str[0]\n",
    "df['us_id'] = df['us_id'].str.split('_').str[1]\n",
    "df['model'] = df['model'].str.replace('/', '-')\n",
    "\n",
    "# Adding a column to mark the experiment number - to map to spreadsheet\n",
    "df['experiment'] = \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reordered = df.iloc[:, [8, 0 , 7, 1, 2 , 3, 4, 5, 6]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reordered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_dir = Path(\"../data/gherkins/sample_data/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reordered.to_csv(exp_dir / \"test_preprocess_input.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get path to active chat log file for a given model\n",
    "# def active_file_path(chat_log_dir, model):\n",
    "#     file_path = chat_log_dir / model / \"active.json\"\n",
    "\n",
    "#     return file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Helper function to find path to the latest chat log for a specific model\n",
    "# def find_latest_log_file(log_dir, model):\n",
    "#     files = [\n",
    "#         f for f in os.listdir(log_dir)\n",
    "#         if f.endswith(\".json\")\n",
    "#     ]\n",
    "\n",
    "#     if files:\n",
    "#         files.sort(reverse=True)\n",
    "#         return os.path.join(log_dir, files[0])\n",
    "    \n",
    "#     return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Helper function to find path to the latest chat log for a specific model\n",
    "# def find_latest_log_file(log_dir, model):\n",
    "#     files = [\n",
    "#         f for f in os.listdir(log_dir)\n",
    "#         if f.endswith(\".json\")\n",
    "#     ]\n",
    "\n",
    "#     if files:\n",
    "#         files.sort(reverse=True)\n",
    "#         return os.path.join(log_dir, files[0])\n",
    "    \n",
    "#     return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #  Helper function to load latest chat log from JSON and rebuild messages\n",
    "# def load_conversation(filepath):\n",
    "#     with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "#         conversation_log = json.load(f)\n",
    "\n",
    "#     messages = []\n",
    "\n",
    "#     # Add system prompt if it exists\n",
    "#     if conversation_log.get(\"system_prompt\"):\n",
    "#         messages.append({\n",
    "#             \"role\": \"system\", \n",
    "#             \"content\": conversation_log[\"system_prompt\"]\n",
    "#         })\n",
    "\n",
    "#     for turn in conversation_log[\"conversation\"]:\n",
    "#         messages.append({\n",
    "#             \"role\": turn[\"role\"], \n",
    "#             \"content\": turn[\"content\"]\n",
    "#         })\n",
    "\n",
    "#     completed_stories = [\n",
    "#         turn[\"us_id\"] for turn in conversation_log[\"conversation\"] \n",
    "#         if turn.get(\"role\") == \"user\" and \"us_id\" in turn\n",
    "#     ]\n",
    "\n",
    "#     return messages, conversation_log, completed_stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def chat_with_model(model,user_stories, or_token, log_dir, system_prompt=None, reminder=None, temperature=0.8, resume=True):\n",
    "    model_log_dir = log_dir / model\n",
    "    os.makedirs(model_log_dir, exist_ok=True)\n",
    "\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {or_token}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "\n",
    "    # Find latest conversation log for this model\n",
    "    # log_file_path = find_latest_conversation(log_dir, model)\n",
    "\n",
    "    # if log_file_path and resume:\n",
    "        # print(f\"Resuming from {os.path.basename(log_file_path)}\")\n",
    "        # messages, conversation_log, completed_stories = load_conversation(log_file_path)\n",
    "\n",
    "    # else:\n",
    "        print(f\"Starting new chat for {model}\")\n",
    "\n",
    "        # Initialise new conversation log, this will be written to file after each turn\n",
    "        conversation_log = {\n",
    "            \"model\": model,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"system_prompt\": system_prompt or \"\",\n",
    "            \"resumed_from\": os.path.relpath(log_file_path) if (log_file_path and resume) else None,\n",
    "            \"conversation\": [],\n",
    "        }\n",
    "\n",
    "        messages = []\n",
    "\n",
    "        if system_prompt:\n",
    "            messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "\n",
    "        completed_stories = []\n",
    "\n",
    "    async with httpx.AsyncClient() as client:\n",
    "        for us_id, story in user_stories.items():\n",
    "            if us_id in completed_stories:\n",
    "                print(f\"Skipping completed story: {us_id}\")\n",
    "                continue\n",
    "\n",
    "            if reminder:\n",
    "                messages.append({\n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": reminder\n",
    "                })\n",
    "\n",
    "                conversation_log[\"conversation\"].append({\n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": reminder\n",
    "                })\n",
    "\n",
    "            messages.append({\n",
    "                \"role\": \"user\", \n",
    "                \"content\": story\n",
    "            })\n",
    "\n",
    "            conversation_log[\"conversation\"].append({\n",
    "                \"role\": \"user\", \n",
    "                \"content\": story, \n",
    "                \"us_id\": us_id\n",
    "            })\n",
    "\n",
    "            # Send request\n",
    "            try:\n",
    "                response = await client.post(\n",
    "                    url=\"https://openrouter.ai/api/v1/chat/completions\",\n",
    "                    headers=headers,\n",
    "                    json={\n",
    "                        \"model\": model,\n",
    "                        \"messages\": messages,\n",
    "                        \"temperature\": temperature,\n",
    "                        \"provider\": {\"data_collection\": \"deny\"},\n",
    "                    },\n",
    "                )\n",
    "                response.raise_for_status()\n",
    "\n",
    "                data = response.json()\n",
    "                reply = data[\"choices\"][0][\"message\"][\"content\"]\n",
    "                created = data.get(\"created\", \"\")\n",
    "\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {us_id}: {e}\")\n",
    "                continue  # Skip but keep loop running\n",
    "\n",
    "            # Add assistant reply\n",
    "            messages.append({\"role\": \"assistant\", \"content\": reply})\n",
    "            conversation_log[\"conversation\"].append(\n",
    "                {\"role\": \"assistant\", \"content\": reply}\n",
    "            )\n",
    "\n",
    "            # Save checkpoint after each turn\n",
    "            save_conversation(log_dir, model, conversation_log)\n",
    "\n",
    "            # Respect rate limits\n",
    "            await asyncio.sleep(1)\n",
    "\n",
    "    print(\n",
    "        f\"Chat completed for {model}. \"\n",
    "        f\"Total turns: {len(conversation_log['conversation'])}\"\n",
    "    )\n",
    "    \n",
    "    return conversation_log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run multi-turn chat for one model, resuming if a saved log exists\n",
    "async def chat_with_model(model, user_stories, or_token):\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {or_token}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    log_file = find_latest_conversation(model)\n",
    "\n",
    "    if log_file:\n",
    "        print(f\"Resuming from {log_file}\")\n",
    "\n",
    "        messages, conversation_log, completed_stories = load_conversation(log_file)\n",
    "    else:\n",
    "        print(f\"Starting new chat for {model}\")\n",
    "\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\", \"content\": (\n",
    "                    prompt\n",
    "            )}\n",
    "        ]\n",
    "        \n",
    "        conversation_log = {\n",
    "            \"model\": model,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"conversation\": []\n",
    "        }\n",
    "\n",
    "        completed_stories = []\n",
    "\n",
    "    async with httpx.AsyncClient() as client:\n",
    "        for us_id, story in user_stories.items():\n",
    "            if us_id in completed_stories:\n",
    "                print(f\"Skipping completed story: {us_id}\")\n",
    "\n",
    "                continue\n",
    "\n",
    "            if reminder:\n",
    "                messages.append({\"role\": \"user\", \"content\": reminder})\n",
    "\n",
    "            messages.append({\"role\": \"user\", \"content\": story})\n",
    "\n",
    "            response = await client.post(\n",
    "                url=\"https://openrouter.ai/api/v1/chat/completions\",\n",
    "                headers=headers,\n",
    "                json={\n",
    "                    \"model\": model,\n",
    "                    \"messages\": messages,\n",
    "                    \"temperature\": 0.8,\n",
    "                    \"provider\": {\n",
    "                        \"data_collection\": \"deny\"\n",
    "                        }\n",
    "                }\n",
    "            )\n",
    "\n",
    "            data = response.json()\n",
    "            reply = data[\"choices\"][0][\"message\"][\"content\"]\n",
    "            messages.append({\"role\": \"assistant\", \"content\": reply})\n",
    "\n",
    "            conversation_log[\"conversation\"].append({\n",
    "                \"us_id\": us_id,\n",
    "                \"user_story\": story,\n",
    "                \"assistant_response\": reply,\n",
    "                \"raw_response\": data\n",
    "            })\n",
    "\n",
    "            save_conversation(conversation_log, model)\n",
    "            await asyncio.sleep(1)  # short pause for rate limits\n",
    "\n",
    "    return conversation_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual prf\n",
    "# Calculate precision, recall, and F1-score\n",
    "prf = []\n",
    "\n",
    "for (metric, model), group in predicted_matches.groupby([\"metric\", \"model\"]):\n",
    "    for us in group[\"true_us\"].unique():\n",
    "\n",
    "        true_pos = ((group[\"predicted_us\"] == us) & (group[\"true_us\"] == us)).sum()\n",
    "        false_pos = ((group[\"predicted_us\"] == us) & (group[\"true_us\"] != us)).sum()\n",
    "\n",
    "        false_neg = ((group[\"predicted_us\"] != us) & (group[\"true_us\"] == us)).sum()\n",
    "\n",
    "        precision = true_pos / (true_pos + false_pos) if (true_pos + false_pos) > 0 else 0\n",
    "        recall = true_pos / (true_pos + false_neg) if (true_pos + false_neg) > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "        prf.append({\n",
    "            \"metric\": metric,\n",
    "            \"model\": model,\n",
    "            \"us_id\": us,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1_score\": f1\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate and save embeddings (us or scenario) to pickle file\n",
    "def generate_and_save_embeddings(ids, texts, embedding_model, filename):\n",
    "    embeddings = embedding_model.encode(texts, show_progress_bar=True)\n",
    "\n",
    "    embeddings_df = pd.DataFrame({\n",
    "        \"id\": ids,\n",
    "        \"embedding\": embeddings.tolist()\n",
    "    })\n",
    "\n",
    "    os.makedirs(experiment_path / 'embeddings', exist_ok=True)\n",
    "    embeddings_df.to_pickle(experiment_path / 'embeddings' / filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate and save US embeddings using each embedding model\n",
    "# for embed_model_name, embed_model in embed_model_dict.items():\n",
    "#     generate_and_save_embeddings(\n",
    "#         us_ids,\n",
    "#         us_texts,\n",
    "#         embed_model,\n",
    "#         f'us_embeddings_{embed_model_name}.pkl'\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "for embed_model_name, embed_model in embed_model_dict.items():\n",
    "    for llm, group in df.groupby(\"model\"):\n",
    "        scenario_ids = group[\"scenario_id\"].tolist()\n",
    "        scenario_texts = group[\"scenario_text\"].tolist()\n",
    "\n",
    "        generate_and_save_embeddings(\n",
    "            scenario_ids,\n",
    "            scenario_texts,\n",
    "            embed_model,\n",
    "            f'{llm}_scenario_embeddings_{embed_model_name}.pkl'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for embed_model_name in embed_model_dict.keys():\n",
    "    # Load US embeddings from pickle\n",
    "    us_embeddings_df = pd.read_pickle(experiment_path / f\"embeddings/us_embeddings_{embed_model_name}.pkl\")\n",
    "\n",
    "    us_ids = us_embeddings_df[\"id\"].tolist()\n",
    "    us_embeddings = np.stack(us_embeddings_df[\"embedding\"].values) # Convert to numpy array for cosine similarity computation\n",
    "\n",
    "    for llm in llms:\n",
    "        # Load scenario embeddings from pickle\n",
    "        scenario_embeddings_df = pd.read_pickle(embeddings_path / f\"{llm}_scenario_embeddings_{embed_model_name}.pkl\")\n",
    "\n",
    "        scenario_ids = scenario_embeddings_df[\"id\"].tolist()\n",
    "        scenario_embeddings = np.stack(scenario_embeddings_df[\"embedding\"].values)\n",
    "\n",
    "        # Compute cosine similarity\n",
    "        cosine_sim_matrix = cosine_similarity(us_embeddings, scenario_embeddings)\n",
    "\n",
    "        # NOTE: we don't preprocess texts for embeddings so preprocessed fields won't be included here\n",
    "        current_results = [{\n",
    "            \"model\": llm, \n",
    "            \"us_id\": us_ids[i],\n",
    "            \"scenario_id\": scenario_ids[j],\n",
    "            \"metric\": f\"{embed_model_name}_embedding_cosine-sim\",\n",
    "            \"similarity_score\": cosine_sim_matrix[i, j]\n",
    "        }\n",
    "        for i in range(len(us_ids)) \n",
    "        for j in range(len(scenario_ids))\n",
    "        ]\n",
    "\n",
    "        results.extend(current_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add us_text and scenario_text to sim_df\n",
    "sim_df = (\n",
    "    sim_df\n",
    "    .merge(df[['scenario_id', 'scenario_text']], on=\"scenario_id\", how=\"left\")\n",
    "    .merge(us_df[['us_id', 'us_text']], on=\"us_id\", how=\"left\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gherkin_eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
