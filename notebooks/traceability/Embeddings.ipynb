{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, sys\n",
    "from pathlib import Path\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "import contractions\n",
    "import spacy\n",
    "\n",
    "# nltk.download(\"wordnet\")\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "from config import DATASET_NAME, EXPERIMENT_NAME, GENERATION_TECHNIQUE\n",
    "\n",
    "# embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up paths and directories\n",
    "experiment_path = Path(f\"../../data/{DATASET_NAME}/experiment_outputs/{EXPERIMENT_NAME}/{GENERATION_TECHNIQUE}/\")\n",
    "\n",
    "us_scen_sim_path = experiment_path / 'similarity_scores'\n",
    "os.makedirs(us_scen_sim_path, exist_ok=True)\n",
    "\n",
    "embeddings_path = experiment_path / 'embeddings'\n",
    "os.makedirs(embeddings_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dict of the embedding models we want to use\n",
    "embed_model_dict = {\n",
    "    \"all-MiniLM-L6-v2\": SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "#### Embed scenario data for scenario--user story traceability experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(experiment_path / 'parsed_scenario_data.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique LLMs used, for looping later\n",
    "llms = df[\"model\"].unique().tolist()\n",
    "\n",
    "llms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "us_df = df[[\"us_id\", \"us_text\"]].drop_duplicates(subset=[\"us_id\"]).reset_index(drop=True)\n",
    "\n",
    "us_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "us_ids = us_df[\"us_id\"].tolist()\n",
    "us_texts = us_df[\"us_text\"].tolist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate and save embeddings (us or scenario) to pickle file\n",
    "def generate_and_save_embeddings(ids, texts, embedding_model, filename):\n",
    "    embeddings = embedding_model.encode(texts, show_progress_bar=True)\n",
    "\n",
    "    embeddings_df = pd.DataFrame({\n",
    "        \"id\": ids,\n",
    "        \"embedding\": embeddings.tolist()\n",
    "    })\n",
    "\n",
    "    os.makedirs(experiment_path / 'embeddings', exist_ok=True)\n",
    "    embeddings_df.to_pickle(experiment_path / 'embeddings' / filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: uncomment below code to generate US embeddings\n",
    "\n",
    "# # Generate and save US embeddings using each embedding model\n",
    "# for embed_model_name, embed_model in embed_model_dict.items():\n",
    "#     generate_and_save_embeddings(\n",
    "#         us_ids,\n",
    "#         us_texts,\n",
    "#         embed_model,\n",
    "#         f'us_embeddings_{embed_model_name}.pkl'\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: uncomment below code to generate scenario embeddings\n",
    "\n",
    "# # Generate and save scenario (each LLM generated set) embeddings using each embedding model\n",
    "# for embed_model_name, embed_model in embed_model_dict.items():\n",
    "#     for llm, group in df.groupby(\"model\"):\n",
    "#         scenario_ids = group[\"scenario_id\"].tolist()\n",
    "#         scenario_texts = group[\"scenario_text\"].tolist()\n",
    "\n",
    "#         generate_and_save_embeddings(\n",
    "#             scenario_ids,\n",
    "#             scenario_texts,\n",
    "#             embed_model,\n",
    "#             f'{llm}_scenario_embeddings_{embed_model_name}.pkl'\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for embed_model_name in embed_model_dict.keys():\n",
    "    # Load US embeddings from pickle\n",
    "    us_embeddings_df = pd.read_pickle(experiment_path / f\"embeddings/us_embeddings_{embed_model_name}.pkl\")\n",
    "\n",
    "    us_ids = us_embeddings_df[\"id\"].tolist()\n",
    "    us_embeddings = np.stack(us_embeddings_df[\"embedding\"].values) # Convert to numpy array for cosine similarity computation\n",
    "\n",
    "    for llm in llms:\n",
    "        # Load scenario embeddings from pickle\n",
    "        scenario_embeddings_df = pd.read_pickle(embeddings_path / f\"{llm}_scenario_embeddings_{embed_model_name}.pkl\")\n",
    "\n",
    "        scenario_ids = scenario_embeddings_df[\"id\"].tolist()\n",
    "        scenario_embeddings = np.stack(scenario_embeddings_df[\"embedding\"].values)\n",
    "\n",
    "        # Compute cosine similarity\n",
    "        cosine_sim_matrix = cosine_similarity(us_embeddings, scenario_embeddings)\n",
    "\n",
    "        # NOTE: we don't preprocess texts for embeddings so preprocessed fields won't be included here\n",
    "        current_results = [{\n",
    "            \"model\": llm, \n",
    "            \"us_id\": us_ids[i],\n",
    "            \"scenario_id\": scenario_ids[j],\n",
    "            \"metric\": f\"{embed_model_name}_embedding_cosine-sim\",\n",
    "            \"similarity_score\": cosine_sim_matrix[i, j]\n",
    "        }\n",
    "        for i in range(len(us_ids)) \n",
    "        for j in range(len(scenario_ids))\n",
    "        ]\n",
    "\n",
    "        results.extend(current_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_df = pd.DataFrame(results)\n",
    "\n",
    "sim_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add us_text and scenario_text to sim_df\n",
    "sim_df = (\n",
    "    sim_df\n",
    "    .merge(df[['scenario_id', 'scenario_text']], on=\"scenario_id\", how=\"left\")\n",
    "    .merge(us_df[['us_id', 'us_text']], on=\"us_id\", how=\"left\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for any missing us_texts after merge\n",
    "sim_df[sim_df[\"us_text\"].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for any missing scenario_texts after merge\n",
    "sim_df[sim_df[\"scenario_text\"].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_df.to_csv(us_scen_sim_path / 'embedding_cosine_similarity_scores.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "#### Embed step data for weighted step embedding traceability experiments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "step_df = pd.read_csv(experiment_path / 'processed_step_data.csv')\n",
    "\n",
    "step_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "for embed_model_name, embed_model in embed_model_dict.items():\n",
    "    for llm, group in step_df.groupby(\"model\"):\n",
    "        step_ids = group[\"step_id\"].tolist()\n",
    "        step_texts = group[\"flat_step\"].tolist()\n",
    "\n",
    "        generate_and_save_embeddings(\n",
    "            step_ids,\n",
    "            step_texts,\n",
    "            embed_model,\n",
    "            f'{llm}_step_embeddings_{embed_model_name}.pkl'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "step_averaged_results = []\n",
    "\n",
    "for embed_model_name in embed_model_dict.keys():\n",
    "    us_embeddings_df = pd.read_pickle(experiment_path / f\"embeddings/us_embeddings_{embed_model_name}.pkl\")\n",
    "\n",
    "    us_ids = us_embeddings_df[\"id\"].tolist()\n",
    "    us_embeddings = np.stack(us_embeddings_df[\"embedding\"].values) # Convert to numpy array for cosine similarity computation\n",
    "\n",
    "    for llm in llms:\n",
    "        step_embeddings_df = pd.read_pickle(embeddings_path / f\"{llm}_step_embeddings_{embed_model_name}.pkl\")\n",
    "\n",
    "        step_embeddings_df[\"scenario_id\"] = step_embeddings_df[\"id\"].str.rsplit(\"_\", n=1).str[0]\n",
    "\n",
    "        # Get scenario embeddings by averaging step embeddings for each scenario\n",
    "        scenario_embeddings_df = step_embeddings_df.groupby(\"scenario_id\")[\"embedding\"].apply(\n",
    "            lambda embeddings_list: np.mean(np.stack(embeddings_list.values), axis=0) # Turn list of step embeddings into numpy array and average column-wise\n",
    "        ).reset_index()\n",
    "\n",
    "        # Normalise the averaged embeddings\n",
    "        scenario_embeddings_df[\"embedding\"] = scenario_embeddings_df[\"embedding\"].apply(\n",
    "            lambda embedding: embedding / np.linalg.norm(embedding) if np.linalg.norm(embedding) != 0 else embedding\n",
    "        )\n",
    "\n",
    "        scenario_ids = scenario_embeddings_df[\"scenario_id\"].tolist()\n",
    "        scenario_embeddings = np.stack(scenario_embeddings_df[\"embedding\"].values)\n",
    "\n",
    "        # step_ids = step_embeddings_df[\"id\"].tolist()\n",
    "        # step_embeddings = np.stack(step_embeddings_df[\"embedding\"].values)\n",
    "\n",
    "        # Compute cosine similarity\n",
    "        cosine_sim_matrix = cosine_similarity(us_embeddings, scenario_embeddings)\n",
    "\n",
    "        current_results = [{\n",
    "            \"model\": llm, \n",
    "            \"us_id\": us_ids[i],\n",
    "            \"scenario_id\": scenario_ids[j],\n",
    "            \"metric\": f\"{embed_model_name}_step_averaged_embedding_cosine-sim\",\n",
    "            \"similarity_score\": cosine_sim_matrix[i, j]\n",
    "        }\n",
    "        for i in range(len(us_ids)) \n",
    "        for j in range(len(scenario_ids))\n",
    "        ]\n",
    "\n",
    "        step_averaged_results.extend(current_results)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "step_averaged_sim_df = pd.DataFrame(step_averaged_results)\n",
    "    \n",
    "step_averaged_sim_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add us_text and scenario_text to step_averaged_sim_df\n",
    "step_averaged_sim_df = (\n",
    "    step_averaged_sim_df\n",
    "    .merge(df[['scenario_id', 'scenario_text']], on=\"scenario_id\", how=\"left\")\n",
    "    .merge(us_df[['us_id', 'us_text']], on=\"us_id\", how=\"left\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "step_averaged_sim_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "step_averaged_sim_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "step_averaged_sim_df.to_csv(us_scen_sim_path / 'step_averaged_embedding_cosine_similarity_scores.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gherkin_eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
