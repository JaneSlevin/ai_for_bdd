{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os, sys\n",
    "from pathlib import Path\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "import contractions\n",
    "import spacy\n",
    "\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "from config import DATASET_NAME, EXPERIMENT_NAME, GENERATION_TECHNIQUE\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support # https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up paths and directories\n",
    "score_files_path = Path(f\"../../data/{DATASET_NAME}/experiment_outputs/{EXPERIMENT_NAME}/{GENERATION_TECHNIQUE}/similarity_scores/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all csv files found in scores_path dir into single df\n",
    "score_files = os.listdir(score_files_path)\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for file in score_files:\n",
    "    current_path = score_files_path / file\n",
    "    current_df = pd.read_csv(current_path)\n",
    "\n",
    "    df = pd.concat([df, current_df]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data for all metrics has been added to df\n",
    "df[\"metric\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract root us_id from scenario_id\n",
    "def parse_root_us(scenario_id):\n",
    "    return scenario_id.split(\"_\")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column identifying root US for each scenario, and a column indicating if current US matches root US\n",
    "df[\"true_us\"] = df[\"scenario_id\"].apply(parse_root_us).astype(str).astype(int)\n",
    "\n",
    "df[\"us_match\"] = df[\"us_id\"] == df[\"true_us\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: metrics are not directly comparable so viewing raw data has limited value\n",
    "# df.groupby([\"metric\", \"model\", \"us_match\"])[\"similarity_score\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "#### Analysis\n",
    "\n",
    "For each `(model, similarity metric)` grouping, the predicted user story for each scenario is the one that achieves the highest similarity score with that scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find US with highest similarity score for each scenario for each metric-model grouping\n",
    "predicted_matches = df.loc[df.groupby([\"metric\", \"model\", \"scenario_id\"])[\"similarity_score\"].idxmax()].rename(columns={\"us_id\" : \"predicted_us\"})\n",
    "\n",
    "predicted_matches.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_matches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHeck if every us_id appears at least once in predicted_matches per (metric, model) grouping\n",
    "all_us_ids = set(df[\"us_id\"].unique())\n",
    "\n",
    "for (metric, model), group in predicted_matches.groupby([\"metric\", \"model\"]):\n",
    "    print(f\"Metric: {metric} \\nModel: {model}\")\n",
    "\n",
    "    predicted_us_ids = set(group[\"predicted_us\"].unique())\n",
    "    missing_us_ids = all_us_ids - predicted_us_ids\n",
    "    print(missing_us_ids, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that every us_id appears at least once in predicted_matches\n",
    "all_us_ids = set(df[\"us_id\"].unique())\n",
    "predicted_us_ids = set(predicted_matches[\"predicted_us\"].unique())\n",
    "\n",
    "missing_us_ids = all_us_ids - predicted_us_ids\n",
    "missing_us_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "Accuracy is calculated for each `(similarity metric, model)` grouping as the proportion of scenarios for which the predicted user story (the one with the highest similarity score) matches the true user story."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As us_match is boolean, mean gives the proportion of true values/the accuracy, i.e., how often the top predicted US matches the true US\n",
    "accuracy = predicted_matches.groupby([\"metric\", \"model\"])[\"us_match\"].mean().reset_index().rename(columns={\"us_match\": \"accuracy\"})\n",
    "\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "\n",
    "Precision, recall, and F1 are calculated per user story based on the scenarios predicted to belong to that story. Macro and weighted averages summarise performance across user stories within each `(similarity metric, model)` grouping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_us_results = []\n",
    "agg_results = []\n",
    "\n",
    "for (metric, model), group in predicted_matches.groupby([\"metric\", \"model\"]): \n",
    "    us_labels = group[\"true_us\"].unique()\n",
    "\n",
    "    accuracy = group[\"us_match\"].mean() # As us_match is boolean, mean gives the proportion of values where the top predicted US matches the true US, i.e., the accuracy\n",
    "\n",
    "    # Compute precision, recall, f1-score for each US (including support, which is the number of ground truth instances for each US)\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(\n",
    "        group[\"true_us\"], group[\"predicted_us\"], labels=us_labels, zero_division=0\n",
    "    )\n",
    "\n",
    "    true_positives = group.loc[group[\"us_match\"], \"true_us\"].value_counts().to_dict() # Count of correctly predicted scenarios per US\n",
    "    predicted_positives = group[\"predicted_us\"].value_counts().to_dict() # Count of predicted scenarios per US\n",
    "\n",
    "    per_us = pd.DataFrame({\n",
    "        \"metric\": metric,\n",
    "        \"model\": model,\n",
    "        \"us_id\": us_labels,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"support\": support,\n",
    "    })\n",
    "\n",
    "    per_us[\"TP_count\"] = per_us[\"us_id\"].map(true_positives).fillna(0).astype(int)\n",
    "    per_us[\"PP_count\"] = per_us[\"us_id\"].map(predicted_positives).fillna(0).astype(int)\n",
    "\n",
    "    \n",
    "    # Compute aggregated metrics -- macro, which gives equal weight to each US, and weighted, which weights by support\n",
    "    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n",
    "        group[\"true_us\"], group[\"predicted_us\"], average=\"macro\", zero_division=0\n",
    "    )\n",
    "\n",
    "    precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(\n",
    "        group[\"true_us\"], group[\"predicted_us\"], average=\"weighted\", zero_division=0\n",
    "    )\n",
    "    \n",
    "    agg = pd.DataFrame({\n",
    "        \"metric\": [metric],\n",
    "        \"model\": [model],\n",
    "        \"accuracy\": [accuracy],\n",
    "        \"precision_macro\": [precision_macro],\n",
    "        \"recall_macro\": [recall_macro],\n",
    "        \"f1_macro\": [f1_macro],\n",
    "        \"precision_weighted\": [precision_weighted],\n",
    "        \"recall_weighted\": [recall_weighted],\n",
    "        \"f1_weighted\": [f1_weighted]\n",
    "    })\n",
    "    \n",
    "    # Combine per-US and aggregated metrics in the results list\n",
    "    per_us_results.append(per_us)\n",
    "    agg_results.append(agg)\n",
    "\n",
    "per_us_df = pd.concat(per_us_results, ignore_index=True)\n",
    "per_us_df = per_us_df.sort_values(by=[\"metric\", \"model\", \"us_id\"]).reset_index(drop=True)\n",
    "\n",
    "agg_df = pd.concat(agg_results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_us_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_us_df[\"diff\"] = per_us_df[\"PP_count\"] - per_us_df[\"support\"]\n",
    "per_us_df[\"ratio\"] = per_us_df[\"PP_count\"] / per_us_df[\"support\"] \n",
    "\n",
    "per_us_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: guiraud's index for lexical richness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gherkin_eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
